{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. ç¯å¢ƒé…ç½®ä¸ä¾èµ–å®‰è£…\n",
        "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£… onnx å’Œ onnxruntimeï¼Œtorchaudio é€šå¸¸å·²é¢„è£…åœ¨ Colab ä¸­ï¼Œä½†ä¸ºäº†ç¡®ä¿ç‰ˆæœ¬å…¼å®¹æ€§ï¼Œæˆ‘ä»¬æ˜¾å¼å®‰è£… CPU ç‰ˆæœ¬ä»¥æ¨¡æ‹Ÿæ‚¨çš„ç›®æ ‡éƒ¨ç½²ç¯å¢ƒ"
      ],
      "metadata": {
        "id": "bR-loRkBSODU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2cLG6LCR9Gh",
        "outputId": "2db14ff2-a214-4494-f7c4-d41696a98124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.19.1)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (1.23.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "PyTorch Version: 2.8.0+cu126\n",
            "ONNX Runtime Device: CPU\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies & Imports\n",
        "# å®‰è£… ONNX ç›¸å…³åº“\n",
        "!pip install onnx onnxruntime\n",
        "# ä¸ºäº†æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒçš„è½»é‡åŒ–ï¼Œæˆ‘ä»¬ä¸å®‰è£… CUDA ç‰ˆæœ¬çš„ torchï¼Œä½†åœ¨ Colab ä¸­ç›´æ¥ä½¿ç”¨é¢„è£…ç‰ˆæœ¬å³å¯\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"ONNX Runtime Device: {ort.get_device()}\")\n",
        "\n",
        "# è®¾å®šéšæœºç§å­ä»¥ä¿è¯å¤ç°æ€§\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. éŸ³é¢‘é¢„å¤„ç†ä¸ MFCC ç‰¹å¾æå–\n",
        "æ ¹æ®æŠ¥å‘Šï¼Œæˆ‘ä»¬å¿…é¡»ä¸¥æ ¼æ§åˆ¶ MFCC çš„å‚æ•°ä»¥ä¿è¯ Python è®­ç»ƒç«¯ä¸ C++ æ¨ç†ç«¯ï¼ˆå¦‚æœæœ‰ï¼‰æˆ–ä¸åŒåº“ä¹‹é—´çš„ä¸€è‡´æ€§ ã€‚æˆ‘ä»¬å°†é‡‡æ ·ç‡ç»Ÿä¸€ä¸º 16kHz"
      ],
      "metadata": {
        "id": "Zt0anwbCSQkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Feature Extraction (Dual Processors)\n",
        "# [cite_start]æ ¸å¿ƒé…ç½®ï¼Œå¿…é¡»ä¸æŠ¥å‘Šä¸€è‡´ [cite: 307]\n",
        "SAMPLE_RATE = 16000\n",
        "N_MFCC = 40\n",
        "N_FFT = 400\n",
        "HOP_LENGTH = 160\n",
        "\n",
        "# --- 1. æ ‡å‡†å¤„ç†å™¨ (ç”¨äºéªŒè¯é›† & å¯¼å‡ºæ¨¡å‹) ---\n",
        "class AudioProcessor:\n",
        "    def __init__(self):\n",
        "        self.mfcc_transform = T.MFCC(\n",
        "            sample_rate=SAMPLE_RATE,\n",
        "            n_mfcc=N_MFCC,\n",
        "            melkwargs={\n",
        "                \"n_fft\": N_FFT,\n",
        "                \"n_mels\": 40,\n",
        "                \"hop_length\": HOP_LENGTH,\n",
        "                \"mel_scale\": \"htk\",\n",
        "            },\n",
        "        )\n",
        "\n",
        "    def process(self, waveform):\n",
        "        # æå– MFCC\n",
        "        mfcc = self.mfcc_transform(waveform)\n",
        "        # [cite_start]æ ‡å‡†åŒ– (Instance Norm) [cite: 61-63]\n",
        "        mean = mfcc.mean(dim=2, keepdim=True)\n",
        "        std = mfcc.std(dim=2, keepdim=True)\n",
        "        return (mfcc - mean) / (std + 1e-6)\n",
        "\n",
        "# --- 2. å¢å¼ºå¤„ç†å™¨ (ä»…ç”¨äºè®­ç»ƒé›†) ---\n",
        "class AugmentedAudioProcessor(AudioProcessor):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # [cite_start]é¢‘åŸŸé®æ©: æ¨¡æ‹Ÿéƒ¨åˆ†é¢‘ç‡ä¸¢å¤± [cite: 245]\n",
        "        self.freq_mask = T.FrequencyMasking(freq_mask_param=10)\n",
        "        # [cite_start]æ—¶åŸŸé®æ©: æ¨¡æ‹ŸéŸ³é¢‘ä¸¢åŒ…æˆ–ç¬æ—¶é™éŸ³ [cite: 247]\n",
        "        self.time_mask = T.TimeMasking(time_mask_param=30)\n",
        "\n",
        "    def process(self, waveform):\n",
        "        # å¤ç”¨çˆ¶ç±»é€»è¾‘æå– MFCC\n",
        "        mfcc = self.mfcc_transform(waveform)\n",
        "\n",
        "        # === å…³é”®å·®å¼‚: åœ¨æ ‡å‡†åŒ–ä¹‹å‰åº”ç”¨ SpecAugment ===\n",
        "        mfcc = self.freq_mask(mfcc)\n",
        "        mfcc = self.time_mask(mfcc)\n",
        "\n",
        "        # æ ‡å‡†åŒ–\n",
        "        mean = mfcc.mean(dim=2, keepdim=True)\n",
        "        std = mfcc.std(dim=2, keepdim=True)\n",
        "        return (mfcc - mean) / (std + 1e-6)\n",
        "\n",
        "# å®ä¾‹åŒ–ä¸¤ä¸ªå¤„ç†å™¨\n",
        "clean_processor = AudioProcessor()\n",
        "aug_processor = AugmentedAudioProcessor()\n",
        "print(\"âœ… Processors Ready: Clean (Validation) & Augmented (Training)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AlX6MBySCUH",
        "outputId": "a7688224-5e5c-4542-cecb-53cc8d5ba25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Processors Ready: Clean (Validation) & Augmented (Training)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. æ„å»ºæ•°æ®é›† (æ¨¡æ‹Ÿ RAVDESS)\n",
        "ä¸ºäº†è®©æ‚¨å¯ä»¥ç«‹å³è¿è¡Œä»£ç ï¼Œæˆ‘ç¼–å†™äº†ä¸€ä¸ªç”Ÿæˆâ€œæ¨¡æ‹Ÿæ•°æ®â€çš„ç±»ã€‚å¦‚æœæ‚¨ä¸‹è½½äº† RAVDESS æ•°æ®é›† ï¼Œåªéœ€ä¿®æ”¹ __getitem__ æ–¹æ³•è¯»å–çœŸå®çš„ .wav æ–‡ä»¶å³å¯ã€‚"
      ],
      "metadata": {
        "id": "WK1CC2ZDSSwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Dataset & Physical Isolation Splitting\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import kagglehub\n",
        "\n",
        "# === A. å‡†å¤‡æ–‡ä»¶åˆ—è¡¨ ===\n",
        "print(\"æ­£åœ¨å‡†å¤‡æ–‡ä»¶åˆ—è¡¨...\")\n",
        "try:\n",
        "    # [cite_start]å°è¯•ä½¿ç”¨ KaggleHub ä¸‹è½½ [cite: 84]\n",
        "    dataset_path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "    all_wav_files = glob.glob(os.path.join(dataset_path, \"**/*.wav\"), recursive=True)\n",
        "    print(f\"âœ… KaggleHub: æ‰¾åˆ° {len(all_wav_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶ã€‚\")\n",
        "except:\n",
        "    # é™çº§åˆ°æœ¬åœ°ç›®å½•\n",
        "    print(\"âš ï¸ KaggleHub å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æœ¬åœ°ç›®å½•...\")\n",
        "    all_wav_files = glob.glob(\"ravdess_data/**/*.wav\", recursive=True)\n",
        "\n",
        "# === B. æ•°æ®é›†ç±»å®šä¹‰ ===\n",
        "class RAVDESSDataset(Dataset):\n",
        "    def __init__(self, file_list, processor, target_len_sec=3):\n",
        "        self.file_paths = file_list\n",
        "        self.processor = processor # æ³¨å…¥æŒ‡å®šçš„å¤„ç†å™¨ (Clean æˆ– Aug)\n",
        "        self.target_sr = 16000\n",
        "        self.target_len = self.target_sr * target_len_sec\n",
        "        self.labels = []\n",
        "\n",
        "        # [cite_start]æƒ…æ„Ÿæ˜ å°„: 0:Neg, 1:Neu, 2:Pos [cite: 100-114]\n",
        "        self.emotion_map = {\n",
        "            '01': 1, '02': 1, # Neutral\n",
        "            '03': 2, '08': 2, # Positive\n",
        "            '04': 0, '05': 0, '06': 0, '07': 0 # Negative\n",
        "        }\n",
        "        self._parse_labels()\n",
        "\n",
        "    def _parse_labels(self):\n",
        "        valid_files = []\n",
        "        valid_labels = []\n",
        "        for path in self.file_paths:\n",
        "            parts = os.path.basename(path).split('-')\n",
        "            if len(parts) == 7:\n",
        "                code = parts[2]\n",
        "                if code in self.emotion_map:\n",
        "                    valid_files.append(path)\n",
        "                    valid_labels.append(self.emotion_map[code])\n",
        "\n",
        "        self.file_paths = valid_files\n",
        "        self.labels = valid_labels # å…¬å¼€ labels ä»¥ä¾¿è®¡ç®—æƒé‡\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # 1. åŠ è½½éŸ³é¢‘ (å…ˆæ‰§è¡Œ Load!)\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(path)\n",
        "        except Exception as e:\n",
        "            # å®¹é”™è¿”å›ç©ºå¼ é‡\n",
        "            return torch.zeros(40, 301), label\n",
        "\n",
        "        # [cite_start]2. Crash Fix: æ£€æŸ¥å¿…é¡»åœ¨ Load ä¹‹å! [cite: 139]\n",
        "        # å¼ºåˆ¶è½¬å•å£°é“ (Mono)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # 3. é‡é‡‡æ ·\n",
        "        if sr != self.target_sr:\n",
        "            resampler = T.Resample(sr, self.target_sr)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # 4. é•¿åº¦å¯¹é½\n",
        "        if waveform.shape[1] > self.target_len:\n",
        "            waveform = waveform[:, :self.target_len]\n",
        "        elif waveform.shape[1] < self.target_len:\n",
        "            pad = self.target_len - waveform.shape[1]\n",
        "            waveform = F.pad(waveform, (0, pad))\n",
        "\n",
        "        # 5. ç‰¹å¾æå– (æ ¹æ®ä¼ å…¥çš„ processor å†³å®šæ˜¯å¦å¢å¼º)\n",
        "        features = self.processor.process(waveform)\n",
        "        if features.shape[0] == 1:\n",
        "            features = features.squeeze(0)\n",
        "\n",
        "        return features, label\n",
        "\n",
        "# === C. ç‰©ç†éš”ç¦»æ‹†åˆ† (Physical Isolation) ===\n",
        "# 1. æ‰“ä¹±æ–‡ä»¶åˆ—è¡¨ (ä¸æ˜¯ Dataset å¯¹è±¡)\n",
        "random.seed(42)\n",
        "random.shuffle(all_wav_files)\n",
        "\n",
        "# 2. åˆ‡åˆ†æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (80/20)\n",
        "split_idx = int(0.8 * len(all_wav_files))\n",
        "train_files = all_wav_files[:split_idx]\n",
        "val_files = all_wav_files[split_idx:]\n",
        "\n",
        "# 3. å®ä¾‹åŒ–ä¸¤ä¸ªå®Œå…¨ç‹¬ç«‹çš„æ•°æ®é›†å¯¹è±¡ (å…³é”®ä¿®å¤!)\n",
        "# è®­ç»ƒé›† -> ç»‘å®š aug_processor (æœ‰å™ªå£°)\n",
        "train_dataset = RAVDESSDataset(train_files, processor=aug_processor)\n",
        "# éªŒè¯é›† -> ç»‘å®š clean_processor (æ— å™ªå£°)\n",
        "val_dataset = RAVDESSDataset(val_files, processor=clean_processor)\n",
        "\n",
        "# 4. åˆ›å»ºåŠ è½½å™¨\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"âœ… æ•°æ®é›†ç‰©ç†éš”ç¦»å®Œæˆ:\")\n",
        "print(f\"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬ (å·²å¯ç”¨ SpecAugment)\")\n",
        "print(f\"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬ (æ•°æ®çº¯å‡€ï¼Œæ— æ±¡æŸ“)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElPTQha0SDj8",
        "outputId": "c3a9439d-b56a-42bf-d310-f7f46862c69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨å‡†å¤‡æ–‡ä»¶åˆ—è¡¨...\n",
            "Using Colab cache for faster access to the 'ravdess-emotional-speech-audio' dataset.\n",
            "âœ… KaggleHub: æ‰¾åˆ° 2880 ä¸ªéŸ³é¢‘æ–‡ä»¶ã€‚\n",
            "âœ… æ•°æ®é›†ç‰©ç†éš”ç¦»å®Œæˆ:\n",
            "   è®­ç»ƒé›†: 2304 æ ·æœ¬ (å·²å¯ç”¨ SpecAugment)\n",
            "   éªŒè¯é›†: 576 æ ·æœ¬ (æ•°æ®çº¯å‡€ï¼Œæ— æ±¡æŸ“)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. è½»é‡çº§ 1D-CNN æ¨¡å‹å®šä¹‰\n",
        "è¿™æ˜¯æ ¹æ®æŠ¥å‘Šä¸­â€œ4.2 å£°å­¦æƒ…æ„Ÿæ¨¡å‹æ¶æ„â€ä¸€èŠ‚ä¸¥æ ¼å®ç°çš„æ¨¡å‹"
      ],
      "metadata": {
        "id": "wX7xM1e2SUnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Lightweight 1D-CNN Model Architecture\n",
        "class AcousticEmotionCNN(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(AcousticEmotionCNN, self).__init__()\n",
        "\n",
        "        # Block 1\n",
        "        # Input: (Batch, 40, Time)\n",
        "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2) # Time dimension reduced by half\n",
        "\n",
        "        # Block 2\n",
        "        # Input: (Batch, 64, Time/2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Global Average Pooling\n",
        "        # å°†å˜é•¿åºåˆ—å‹ç¼©ä¸ºå®šé•¿å‘é‡ [cite: 96]\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classifier\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (Batch, 40, Time)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        x = self.global_pool(x) # Output: (Batch, 128, 1)\n",
        "        x = x.squeeze(-1)       # Output: (Batch, 128)\n",
        "\n",
        "        x = self.fc(x)          # Output: (Batch, 3)\n",
        "        return x\n",
        "\n",
        "model = AcousticEmotionCNN()\n",
        "# æ‰“å°å‚æ•°é‡ï¼Œç¡®ä¿å®ƒæ˜¯è½»é‡çº§çš„ (KBçº§åˆ«) [cite: 98]\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "# é¢„æœŸå‚æ•°é‡éå¸¸å°ï¼Œå¤§çº¦åœ¨ 30k-40k å·¦å³"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNaaQeqqSE3S",
        "outputId": "58801858-a22d-4df6-a9a4-066265dfbaf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 33219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. è®­ç»ƒæ¨¡å‹\n",
        "ä¸€ä¸ªæ ‡å‡†çš„ PyTorch è®­ç»ƒå¾ªç¯ã€‚"
      ],
      "metadata": {
        "id": "_Tv_c_xYSWqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Production Training Loop (Final Fix)\n",
        "import copy\n",
        "\n",
        "# --- A. è®¡ç®—ç±»åˆ«æƒé‡ (è§£å†³ä¸å¹³è¡¡) ---\n",
        "y_train = train_dataset.labels\n",
        "class_counts = torch.bincount(torch.tensor(y_train))\n",
        "total_samples = len(y_train)\n",
        "\n",
        "# æƒé‡å…¬å¼: Total / (Classes * Count)\n",
        "class_weights = total_samples / (3 * class_counts.float())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "print(f\"Class Counts (Train): {class_counts.tolist()}\")\n",
        "print(f\"Class Weights applied: {class_weights.tolist()}\")\n",
        "\n",
        "# --- B. è®­ç»ƒé…ç½® ---\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ä¿®å¤: ç§»é™¤ verbose=Trueï¼Œé˜²æ­¢æŠ¥é”™\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "def train_production(num_epochs=20):\n",
        "    print(f\"Starting Production Training on {device}...\")\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_path = 'best_acoustic_model.pth'\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # 1. è®­ç»ƒé˜¶æ®µ\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # 2. éªŒè¯é˜¶æ®µ\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for v_in, v_lab in val_loader:\n",
        "                v_in, v_lab = v_in.to(device), v_lab.to(device)\n",
        "                v_out = model(v_in)\n",
        "                loss = criterion(v_out, v_lab)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, v_pred = torch.max(v_out.data, 1)\n",
        "                val_total += v_lab.size(0)\n",
        "                val_correct += (v_pred == v_lab).sum().item()\n",
        "\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # æ‰‹åŠ¨è·å–å½“å‰å­¦ä¹ ç‡ç”¨äºæ‰“å°\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # --- 3. ä¿å­˜æœ€ä½³æ¨¡å‹ ---\n",
        "        if val_acc > best_val_acc:\n",
        "            print(f\"ğŸ”¥ New Best Model! Val Acc: {best_val_acc:.1f}% -> {val_acc:.1f}%\")\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "        # 4. æ‰“å°æ—¥å¿— (åŒ…å« LR)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.1f}% | \"\n",
        "              f\"Val Acc: {val_acc:.1f}% | \"\n",
        "              f\"LR: {current_lr:.6f}\")\n",
        "\n",
        "    print(f\"\\nâœ… è®­ç»ƒç»“æŸã€‚å†å²æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1f}%\")\n",
        "    print(f\"ğŸ”„ æ­£åœ¨åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡ ({best_model_path})...\")\n",
        "\n",
        "    # --- 4. å›æ»šæƒé‡ ---\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    print(\"âœ… æ¨¡å‹å·²æ¢å¤è‡³æœ€ä½³çŠ¶æ€ï¼Œå‡†å¤‡å¯¼å‡ºã€‚\")\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "train_production(num_epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoKtV8auSGB6",
        "outputId": "d7fb2006-07dc-4f37-ab75-429f9ceaab69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Counts (Train): [1215, 472, 617]\n",
            "Class Weights applied: [0.6320987939834595, 1.6271185874938965, 1.2447326183319092]\n",
            "Starting Production Training on cpu...\n",
            "ğŸ”¥ New Best Model! Val Acc: 0.0% -> 48.3%\n",
            "Epoch 1/20 | Loss: 0.9668 | Train Acc: 43.6% | Val Acc: 48.3% | LR: 0.001000\n",
            "ğŸ”¥ New Best Model! Val Acc: 48.3% -> 62.7%\n",
            "Epoch 2/20 | Loss: 0.8287 | Train Acc: 58.4% | Val Acc: 62.7% | LR: 0.001000\n",
            "ğŸ”¥ New Best Model! Val Acc: 62.7% -> 71.9%\n",
            "Epoch 3/20 | Loss: 0.7239 | Train Acc: 64.4% | Val Acc: 71.9% | LR: 0.001000\n",
            "Epoch 4/20 | Loss: 0.6146 | Train Acc: 70.5% | Val Acc: 71.4% | LR: 0.001000\n",
            "ğŸ”¥ New Best Model! Val Acc: 71.9% -> 74.5%\n",
            "Epoch 5/20 | Loss: 0.5256 | Train Acc: 75.4% | Val Acc: 74.5% | LR: 0.001000\n",
            "ğŸ”¥ New Best Model! Val Acc: 74.5% -> 81.6%\n",
            "Epoch 6/20 | Loss: 0.4441 | Train Acc: 80.6% | Val Acc: 81.6% | LR: 0.001000\n",
            "ğŸ”¥ New Best Model! Val Acc: 81.6% -> 82.1%\n",
            "Epoch 7/20 | Loss: 0.4013 | Train Acc: 83.0% | Val Acc: 82.1% | LR: 0.001000\n",
            "Epoch 8/20 | Loss: 0.3633 | Train Acc: 85.3% | Val Acc: 80.9% | LR: 0.001000\n",
            "ğŸ”¥ New Best Model! Val Acc: 82.1% -> 88.0%\n",
            "Epoch 9/20 | Loss: 0.3268 | Train Acc: 86.3% | Val Acc: 88.0% | LR: 0.001000\n",
            "ğŸ”¥ New Best Model! Val Acc: 88.0% -> 88.2%\n",
            "Epoch 10/20 | Loss: 0.2864 | Train Acc: 88.4% | Val Acc: 88.2% | LR: 0.001000\n",
            "Epoch 11/20 | Loss: 0.2644 | Train Acc: 90.0% | Val Acc: 79.3% | LR: 0.001000\n",
            "Epoch 12/20 | Loss: 0.2672 | Train Acc: 88.7% | Val Acc: 84.7% | LR: 0.001000\n",
            "ğŸ”¥ New Best Model! Val Acc: 88.2% -> 88.7%\n",
            "Epoch 13/20 | Loss: 0.2387 | Train Acc: 90.4% | Val Acc: 88.7% | LR: 0.000500\n",
            "ğŸ”¥ New Best Model! Val Acc: 88.7% -> 90.8%\n",
            "Epoch 14/20 | Loss: 0.1958 | Train Acc: 92.4% | Val Acc: 90.8% | LR: 0.000500\n",
            "ğŸ”¥ New Best Model! Val Acc: 90.8% -> 92.4%\n",
            "Epoch 15/20 | Loss: 0.1806 | Train Acc: 93.4% | Val Acc: 92.4% | LR: 0.000500\n",
            "Epoch 16/20 | Loss: 0.1647 | Train Acc: 94.0% | Val Acc: 91.7% | LR: 0.000500\n",
            "ğŸ”¥ New Best Model! Val Acc: 92.4% -> 95.5%\n",
            "Epoch 17/20 | Loss: 0.1623 | Train Acc: 94.4% | Val Acc: 95.5% | LR: 0.000500\n",
            "Epoch 18/20 | Loss: 0.1703 | Train Acc: 93.4% | Val Acc: 93.8% | LR: 0.000500\n",
            "Epoch 19/20 | Loss: 0.1651 | Train Acc: 94.2% | Val Acc: 91.8% | LR: 0.000500\n",
            "Epoch 20/20 | Loss: 0.1410 | Train Acc: 95.5% | Val Acc: 94.1% | LR: 0.000500\n",
            "\n",
            "âœ… è®­ç»ƒç»“æŸã€‚å†å²æœ€ä½³éªŒè¯å‡†ç¡®ç‡: 95.5%\n",
            "ğŸ”„ æ­£åœ¨åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡ (best_acoustic_model.pth)...\n",
            "âœ… æ¨¡å‹å·²æ¢å¤è‡³æœ€ä½³çŠ¶æ€ï¼Œå‡†å¤‡å¯¼å‡ºã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. å¯¼å‡ºä¸º ONNX å¹¶è¿›è¡Œ Int8 é‡åŒ–\n",
        "è¿™æ˜¯å®ç°æŠ¥å‘Šä¸­â€œStage 2 éªŒæ”¶æ ‡å‡†â€çš„å…³é”®ï¼šæ¨¡å‹å¯¼å‡ºä¸é‡åŒ–"
      ],
      "metadata": {
        "id": "lQpCGJ-iSYSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. ONNX Export & Quantization\n",
        "import onnxruntime.quantization as ohq\n",
        "\n",
        "# 1. å¯¼å‡ºä¸ºæ ‡å‡† ONNX (Float32)\n",
        "# åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿè¾“å…¥ï¼ŒTime ç»´åº¦å¯ä»¥æ˜¯ä»»æ„é•¿åº¦\n",
        "dummy_input = torch.randn(1, 40, 300, device=device)\n",
        "onnx_path = \"acoustic_emotion_fp32.onnx\"\n",
        "quantized_path = \"acoustic_emotion_int8.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=13,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={'input': {2: 'time_steps'}} # å…è®¸è¾“å…¥éŸ³é¢‘é•¿åº¦å˜åŒ– [cite: 96]\n",
        ")\n",
        "print(f\"Model exported to {onnx_path}\")\n",
        "\n",
        "# 2. åŠ¨æ€é‡åŒ– (Dynamic Quantization) -> Int8\n",
        "# è¿™ç§é‡åŒ–æ–¹å¼å¯¹ CPU æ¨ç†æœ€å‹å¥½ï¼Œæ— éœ€æ ¡å‡†æ•°æ® [cite: 134]\n",
        "ohq.quantize_dynamic(\n",
        "    model_input=onnx_path,\n",
        "    model_output=quantized_path,\n",
        "    weight_type=ohq.QuantType.QUInt8 # æƒé‡é‡åŒ–ä¸º 8 ä½æ— ç¬¦å·æ•´æ•°\n",
        ")\n",
        "\n",
        "print(f\"Model quantized to {quantized_path}\")\n",
        "print(f\"Original Size: {os.path.getsize(onnx_path)/1024:.2f} KB\")\n",
        "print(f\"Quantized Size: {os.path.getsize(quantized_path)/1024:.2f} KB\")\n",
        "# é¢„æœŸé‡åŒ–åä½“ç§¯å‡å°‘ 2-4 å€ [cite: 111]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6xv4ys_SHTn",
        "outputId": "3f14388e-1c8d-4224-d998-f6f45a10e285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1106427217.py:10: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n",
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to acoustic_emotion_fp32.onnx\n",
            "Model quantized to acoustic_emotion_int8.onnx\n",
            "Original Size: 129.53 KB\n",
            "Quantized Size: 37.14 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. æ¨ç†åŸºå‡†æµ‹è¯• (PyTorch vs ONNX Int8)\n",
        "æœ€åï¼Œæˆ‘ä»¬éªŒè¯æ€§èƒ½æ˜¯å¦æ»¡è¶³â€œæ¨ç†å»¶è¿Ÿ < 20msâ€çš„æ ‡å‡†"
      ],
      "metadata": {
        "id": "Mwru8O0LSaMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Benchmark: Latency Test\n",
        "def benchmark_inference(infer_func, input_data, n_loops=100):\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(n_loops):\n",
        "        infer_func(input_data)\n",
        "    end = time.perf_counter()\n",
        "    return (end - start) / n_loops * 1000 # ms\n",
        "\n",
        "# å‡†å¤‡æµ‹è¯•æ•°æ® (CPU)\n",
        "test_input_tensor = torch.randn(1, 40, 300) # æ¨¡æ‹Ÿ 3ç§’éŸ³é¢‘\n",
        "test_input_numpy = test_input_tensor.numpy()\n",
        "\n",
        "# 1. PyTorch CPU æ¨ç†\n",
        "model.cpu()\n",
        "model.eval()\n",
        "def run_pytorch(x):\n",
        "    with torch.no_grad():\n",
        "        model(x)\n",
        "\n",
        "# 2. ONNX Runtime Int8 æ¨ç†\n",
        "ort_session = ort.InferenceSession(quantized_path, providers=['CPUExecutionProvider'])\n",
        "def run_onnx(x):\n",
        "    ort_session.run(None, {'input': x})\n",
        "\n",
        "# è¿è¡Œæµ‹è¯•\n",
        "print(\"Running Benchmarks (CPU)...\")\n",
        "pytorch_latency = benchmark_inference(run_pytorch, test_input_tensor)\n",
        "onnx_latency = benchmark_inference(run_onnx, test_input_numpy)\n",
        "\n",
        "print(f\"PyTorch (CPU) Latency: {pytorch_latency:.2f} ms\")\n",
        "print(f\"ONNX Int8 (CPU) Latency: {onnx_latency:.2f} ms\")\n",
        "\n",
        "# éªŒè¯æ˜¯å¦æ»¡è¶³éªŒæ”¶æ ‡å‡†\n",
        "if onnx_latency < 20:\n",
        "    print(\"\\nâœ… SUCCESS: Latency is under 20ms \")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ WARNING: Optimization needed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAg-Jm1ZSIfF",
        "outputId": "765c6b34-2f4e-480a-e525-5bc9bdd70a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Benchmarks (CPU)...\n",
            "PyTorch (CPU) Latency: 0.90 ms\n",
            "ONNX Int8 (CPU) Latency: 2.12 ms\n",
            "\n",
            "âœ… SUCCESS: Latency is under 20ms \n"
          ]
        }
      ]
    }
  ]
}